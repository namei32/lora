<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/neu_det_pipeline/generation.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/neu_det_pipeline/generation.py" />
              <option name="originalContent" value="from __future__ import annotations&#10;&#10;import os&#10;from pathlib import Path&#10;from typing import Dict, List, Sequence, Optional&#10;&#10;import torch&#10;from diffusers import ControlNetModel, StableDiffusionControlNetPipeline&#10;from diffusers.schedulers import (&#10;    DDIMScheduler,&#10;    DPMSolverMultistepScheduler,&#10;    EulerAncestralDiscreteScheduler,&#10;    EulerDiscreteScheduler,&#10;)&#10;from PIL import Image&#10;from rich.progress import track&#10;from safetensors.torch import load_file&#10;&#10;from .config import GenerationConfig&#10;from .lora_train import AttnProcsLayers, get_lora_processor_class, _estimate_hidden_size  # ensures Module subclass even on older diffusers&#10;&#10;SCHEDULER_REGISTRY = {&#10;    &quot;DPMSolverMultistepScheduler&quot;: DPMSolverMultistepScheduler,&#10;    &quot;EulerDiscreteScheduler&quot;: EulerDiscreteScheduler,&#10;    &quot;EulerAncestralDiscreteScheduler&quot;: EulerAncestralDiscreteScheduler,&#10;    &quot;DDIMScheduler&quot;: DDIMScheduler,&#10;}&#10;&#10;&#10;class ControlNetGenerator:&#10;    def __init__(self, cfg: GenerationConfig):&#10;        self.cfg = cfg&#10;&#10;    # ------------------------------------------------------------------&#10;    # Loading helpers&#10;    # ------------------------------------------------------------------&#10;    def _auth_token(self) -&gt; Optional[str]:&#10;        return self.cfg.hf_token or os.getenv(&quot;HF_TOKEN&quot;)&#10;&#10;    def _load_controlnets(self) -&gt; List[ControlNetModel]:&#10;        models: List[ControlNetModel] = []&#10;        for repo_id in track(self.cfg.controlnet_models, description=&quot;Loading ControlNets&quot;):&#10;            try:&#10;                models.append(&#10;                    ControlNetModel.from_pretrained(&#10;                        repo_id,&#10;                        torch_dtype=torch.float16,&#10;                        use_auth_token=self._auth_token(),&#10;                    )&#10;                )&#10;            except OSError as exc:&#10;                hint = (&#10;                    &quot;请确认 ControlNet 模型已下载或设置 HF_TOKEN，以便 diffusers 可以访问对应权重。&quot;&#10;                )&#10;                raise OSError(f&quot;无法加载 ControlNet '{repo_id}': {exc}. {hint}&quot;) from exc&#10;        return models&#10;&#10;    def _build_base_pipe(self, control_nets: Sequence[ControlNetModel]) -&gt; StableDiffusionControlNetPipeline:&#10;        try:&#10;            pipe = StableDiffusionControlNetPipeline.from_pretrained(&#10;                self.cfg.base_model,&#10;                controlnet=list(control_nets),&#10;                torch_dtype=torch.float16,&#10;                safety_checker=None,&#10;                use_auth_token=self._auth_token(),&#10;            )&#10;        except OSError as exc:&#10;            hint = &quot;请确保 base_model 指向合法的 Stable Diffusion 目录 (含 model_index.json / unet / vae 等)。&quot;&#10;            raise OSError(f&quot;无法加载基础模型 '{self.cfg.base_model}': {exc}. {hint}&quot;) from exc&#10;        return pipe&#10;&#10;    def _configure_scheduler(self, pipe: StableDiffusionControlNetPipeline) -&gt; None:&#10;        scheduler_cls = SCHEDULER_REGISTRY.get(self.cfg.scheduler)&#10;        if scheduler_cls is None:&#10;            available = &quot;, &quot;.join(SCHEDULER_REGISTRY)&#10;            raise ValueError(f&quot;未知 scheduler: {self.cfg.scheduler}. 可选值: {available}&quot;)&#10;        pipe.scheduler = scheduler_cls.from_config(pipe.scheduler.config)&#10;&#10;    def _ensure_module_processors(self, pipe: StableDiffusionControlNetPipeline) -&gt; None:&#10;        &quot;&quot;&quot;Ensures every attention processor registered on the UNet is a Module.&quot;&quot;&quot;&#10;        updated: Dict[str, torch.nn.Module] = {}&#10;        needs_update = False&#10;&#10;        class _AttnModuleWrapper(torch.nn.Module):&#10;            def __init__(self, processor):&#10;                super().__init__()&#10;                self.processor = processor&#10;&#10;            def forward(self, *args, **kwargs):  # pragma: no cover - simple proxy&#10;                return self.processor(*args, **kwargs)&#10;&#10;        for name, processor in pipe.unet.attn_processors.items():&#10;            if isinstance(processor, torch.nn.Module):&#10;                updated[name] = processor&#10;                continue&#10;            updated[name] = _AttnModuleWrapper(processor)&#10;            needs_update = True&#10;&#10;        if needs_update:&#10;            pipe.unet.set_attn_processor(updated)&#10;&#10;    def _load_lora(self, pipe: StableDiffusionControlNetPipeline, lora_path: Path) -&gt; None:&#10;        lora_path = Path(lora_path)&#10;        if not lora_path.exists():&#10;            raise FileNotFoundError(f&quot;未找到 LoRA 权重文件: {lora_path}&quot;)&#10;        repo_dir = lora_path.parent&#10;        weight_name = lora_path.name&#10;        last_error: Optional[Exception] = None&#10;        # Try native diffusers helpers first (handles adapters exported elsewhere)&#10;        try:&#10;            pipe.load_lora_weights(str(repo_dir), weight_name=weight_name)&#10;            return&#10;        except Exception as exc:  # noqa: BLE001 - capture for fallback&#10;            last_error = exc&#10;        try:&#10;            pipe.unet.load_attn_procs(str(repo_dir), weight_name=weight_name)&#10;            return&#10;        except Exception as exc:  # noqa: BLE001&#10;            last_error = exc&#10;        # Fallback: state dict injection for weights produced by our trainer&#10;        try:&#10;            state = load_file(str(lora_path), device=&quot;cpu&quot;)&#10;&#10;            # Reconstruct the attention processors from the state dict&#10;            attn_procs: Dict[str, torch.nn.Module] = {}&#10;            lora_cls = get_lora_processor_class()&#10;&#10;            # Use actual processor names from the UNet, not config&#10;            attn_processor_keys = list(pipe.unet.attn_processors.keys())&#10;&#10;            for name in attn_processor_keys:&#10;                cross_attention_dim = pipe.unet.config.cross_attention_dim if name.endswith(&quot;attn2.processor&quot;) else None&#10;                # Estimate sizes from saved tensors&#10;                proc_state_dict = {}&#10;                prefix = name.replace(&quot;.&quot;, &quot;_&quot;) + &quot;.&quot;&#10;                for key, value in state.items():&#10;                    if key.startswith(prefix):&#10;                        new_key = key[len(prefix):]&#10;                        proc_state_dict[new_key] = value&#10;&#10;                if not proc_state_dict:&#10;                    continue&#10;&#10;                # Infer rank and dims&#10;                inferred_rank: Optional[int] = None&#10;                inferred_hidden: Optional[int] = None&#10;                inferred_cross: Optional[int] = None&#10;                for k, v in proc_state_dict.items():&#10;                    if not hasattr(v, &quot;shape&quot;):&#10;                        continue&#10;                    if k.endswith(&quot;down.weight&quot;) and len(v.shape) == 2:&#10;                        r, dim = int(v.shape[0]), int(v.shape[1])&#10;                        inferred_rank = inferred_rank or r&#10;                        # Decide which projection this belongs to by substring&#10;                        lower = k.lower()&#10;                        if &quot;_q.&quot; in lower or &quot;to_q&quot; in lower:&#10;                            inferred_hidden = inferred_hidden or dim&#10;                        elif &quot;_k.&quot; in lower or &quot;to_k&quot; in lower:&#10;                            inferred_cross = inferred_cross or dim&#10;                        elif &quot;_v.&quot; in lower or &quot;to_v&quot; in lower:&#10;                            inferred_cross = inferred_cross or dim&#10;                        elif &quot;out.&quot; in lower or &quot;to_out&quot; in lower:&#10;                            inferred_hidden = inferred_hidden or dim&#10;                rank = inferred_rank or 4&#10;                hidden_size = inferred_hidden or _estimate_hidden_size(name, pipe.unet.config.block_out_channels)&#10;                cross_dim = inferred_cross if (inferred_cross is not None) else cross_attention_dim&#10;&#10;                # Create processor with inferred dimensions&#10;                try:&#10;                    lora_processor = lora_cls(&#10;                        hidden_size=hidden_size,&#10;                        cross_attention_dim=cross_dim,&#10;                        rank=rank,&#10;                        alpha=rank,&#10;                    )&#10;                except TypeError:&#10;                    lora_processor = lora_cls(&#10;                        hidden_size=hidden_size,&#10;                        cross_attention_dim=cross_dim,&#10;                        r=rank,  # type: ignore[arg-type]&#10;                        network_alpha=rank,  # type: ignore[arg-type]&#10;                    )&#10;&#10;                lora_processor.load_state_dict(proc_state_dict, strict=False)&#10;                attn_procs[name] = lora_processor&#10;&#10;            if not attn_procs:&#10;                raise RuntimeError(&quot;Could not find any valid LoRA layers in the state dictionary.&quot;)&#10;&#10;            pipe.unet.set_attn_processor(attn_procs)&#10;&#10;        except Exception as exc:  # noqa: BLE001&#10;            raise RuntimeError(&#10;                f&quot;无法加载 LoRA 权重 {lora_path}: {exc}\n最近一次 diffusers 加载错误: {last_error}&quot;&#10;            ) from exc&#10;&#10;    def build_pipeline(self, lora_path: Path) -&gt; StableDiffusionControlNetPipeline:&#10;        control_nets = self._load_controlnets()&#10;        pipe = self._build_base_pipe(control_nets)&#10;        self._configure_scheduler(pipe)&#10;        self._load_lora(pipe, lora_path)&#10;        pipe.enable_model_cpu_offload()&#10;        return pipe&#10;&#10;    # ------------------------------------------------------------------&#10;    # Conditioning utilities&#10;    # ------------------------------------------------------------------&#10;    def _ensure_lengths(self) -&gt; None:&#10;        n = len(self.cfg.controlnet_models)&#10;        if len(self.cfg.controlnet_modalities) != n:&#10;            raise ValueError(&quot;controlnet_modalities 数量需与 controlnet_models 保持一致。&quot;)&#10;        if len(self.cfg.control_scales) != n:&#10;            raise ValueError(&quot;control_scales 数量需与 controlnet_models 保持一致。&quot;)&#10;&#10;    def _load_condition_images(&#10;        self,&#10;        stem: str,&#10;        conditioning: Dict[str, Dict[str, Path]],&#10;    ) -&gt; List[Image.Image]:&#10;        if stem not in conditioning:&#10;            raise KeyError(f&quot;缺少样本 {stem} 的引导文件。&quot;)&#10;        images: List[Image.Image] = []&#10;        entry = conditioning[stem]&#10;        for modality in self.cfg.controlnet_modalities:&#10;            if modality not in entry:&#10;                raise KeyError(f&quot;样本 {stem} 缺少模态 {modality} 的引导图。&quot;)&#10;            images.append(Image.open(entry[modality]).convert(&quot;RGB&quot;))&#10;        return images&#10;&#10;    # ------------------------------------------------------------------&#10;    # Public API&#10;    # ------------------------------------------------------------------&#10;    def generate(&#10;        self,&#10;        lora_path: Path,&#10;        prompt_items: List[tuple[str, str]],&#10;        conditioning: Dict[str, Dict[str, Path]],&#10;        output_dir: Path,&#10;    ) -&gt; List[Path]:&#10;        self._ensure_lengths()&#10;        pipe = self.build_pipeline(lora_path)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;        results: List[Path] = []&#10;        for stem, prompt in track(prompt_items, description=&quot;Generating&quot;):&#10;            ctrl_images = self._load_condition_images(stem, conditioning)&#10;            # If only one ControlNet is in use, we must pass a single image instead of a list&#10;            cond_image = ctrl_images[0] if len(ctrl_images) == 1 else ctrl_images&#10;            image = pipe(&#10;                prompt,&#10;                num_inference_steps=self.cfg.num_inference_steps,&#10;                guidance_scale=self.cfg.guidance_scale,&#10;                image=cond_image,&#10;                controlnet_conditioning_scale=self.cfg.control_scales,&#10;            ).images[0]&#10;            out_path = output_dir / f&quot;{stem}.png&quot;&#10;            image.save(out_path)&#10;            results.append(out_path)&#10;&#10;        pipe.to(&quot;cpu&quot;)&#10;        torch.cuda.empty_cache()&#10;        return results&#10;" />
              <option name="updatedContent" value="from __future__ import annotations&#10;&#10;import os&#10;from pathlib import Path&#10;from typing import Dict, List, Sequence, Optional&#10;&#10;import torch&#10;from diffusers import ControlNetModel, StableDiffusionControlNetPipeline&#10;from diffusers.schedulers import (&#10;    DDIMScheduler,&#10;    DPMSolverMultistepScheduler,&#10;    EulerAncestralDiscreteScheduler,&#10;    EulerDiscreteScheduler,&#10;)&#10;from PIL import Image&#10;from rich.progress import track&#10;from safetensors.torch import load_file&#10;&#10;from .config import GenerationConfig&#10;from .lora_train import AttnProcsLayers, get_lora_processor_class, _estimate_hidden_size  # ensures Module subclass even on older diffusers&#10;&#10;SCHEDULER_REGISTRY = {&#10;    &quot;DPMSolverMultistepScheduler&quot;: DPMSolverMultistepScheduler,&#10;    &quot;EulerDiscreteScheduler&quot;: EulerDiscreteScheduler,&#10;    &quot;EulerAncestralDiscreteScheduler&quot;: EulerAncestralDiscreteScheduler,&#10;    &quot;DDIMScheduler&quot;: DDIMScheduler,&#10;}&#10;&#10;&#10;class ControlNetGenerator:&#10;    def __init__(self, cfg: GenerationConfig):&#10;        self.cfg = cfg&#10;&#10;    # ------------------------------------------------------------------&#10;    # Loading helpers&#10;    # ------------------------------------------------------------------&#10;    def _auth_token(self) -&gt; Optional[str]:&#10;        return self.cfg.hf_token or os.getenv(&quot;HF_TOKEN&quot;)&#10;&#10;    def _load_controlnets(self) -&gt; List[ControlNetModel]:&#10;        models: List[ControlNetModel] = []&#10;        for repo_id in track(self.cfg.controlnet_models, description=&quot;Loading ControlNets&quot;):&#10;            try:&#10;                models.append(&#10;                    ControlNetModel.from_pretrained(&#10;                        repo_id,&#10;                        torch_dtype=torch.float16,&#10;                        use_auth_token=self._auth_token(),&#10;                    )&#10;                )&#10;            except OSError as exc:&#10;                hint = (&#10;                    &quot;请确认 ControlNet 模型已下载或设置 HF_TOKEN，以便 diffusers 可以访问对应权重。&quot;&#10;                )&#10;                raise OSError(f&quot;无法加载 ControlNet '{repo_id}': {exc}. {hint}&quot;) from exc&#10;        return models&#10;&#10;    def _build_base_pipe(self, control_nets: Sequence[ControlNetModel]) -&gt; StableDiffusionControlNetPipeline:&#10;        try:&#10;            pipe = StableDiffusionControlNetPipeline.from_pretrained(&#10;                self.cfg.base_model,&#10;                controlnet=list(control_nets),&#10;                torch_dtype=torch.float16,&#10;                safety_checker=None,&#10;                use_auth_token=self._auth_token(),&#10;            )&#10;        except OSError as exc:&#10;            hint = &quot;请确保 base_model 指向合法的 Stable Diffusion 目录 (含 model_index.json / unet / vae 等)。&quot;&#10;            raise OSError(f&quot;无法加载基础模型 '{self.cfg.base_model}': {exc}. {hint}&quot;) from exc&#10;        return pipe&#10;&#10;    def _configure_scheduler(self, pipe: StableDiffusionControlNetPipeline) -&gt; None:&#10;        scheduler_cls = SCHEDULER_REGISTRY.get(self.cfg.scheduler)&#10;        if scheduler_cls is None:&#10;            available = &quot;, &quot;.join(SCHEDULER_REGISTRY)&#10;            raise ValueError(f&quot;未知 scheduler: {self.cfg.scheduler}. 可选值: {available}&quot;)&#10;        pipe.scheduler = scheduler_cls.from_config(pipe.scheduler.config)&#10;&#10;    def _ensure_module_processors(self, pipe: StableDiffusionControlNetPipeline) -&gt; None:&#10;        &quot;&quot;&quot;Ensures every attention processor registered on the UNet is a Module.&quot;&quot;&quot;&#10;        updated: Dict[str, torch.nn.Module] = {}&#10;        needs_update = False&#10;&#10;        class _AttnModuleWrapper(torch.nn.Module):&#10;            def __init__(self, processor):&#10;                super().__init__()&#10;                self.processor = processor&#10;&#10;            def forward(self, *args, **kwargs):  # pragma: no cover - simple proxy&#10;                return self.processor(*args, **kwargs)&#10;&#10;        for name, processor in pipe.unet.attn_processors.items():&#10;            if isinstance(processor, torch.nn.Module):&#10;                updated[name] = processor&#10;                continue&#10;            updated[name] = _AttnModuleWrapper(processor)&#10;            needs_update = True&#10;&#10;        if needs_update:&#10;            pipe.unet.set_attn_processor(updated)&#10;&#10;    def _load_lora(self, pipe: StableDiffusionControlNetPipeline, lora_path: Path) -&gt; None:&#10;        lora_path = Path(lora_path)&#10;        if not lora_path.exists():&#10;            raise FileNotFoundError(f&quot;未找到 LoRA 权重文件: {lora_path}&quot;)&#10;        repo_dir = lora_path.parent&#10;        weight_name = lora_path.name&#10;        last_error: Optional[Exception] = None&#10;        # Try native diffusers helpers first (handles adapters exported elsewhere)&#10;        try:&#10;            pipe.load_lora_weights(str(repo_dir), weight_name=weight_name)&#10;            return&#10;        except Exception as exc:  # noqa: BLE001 - capture for fallback&#10;            last_error = exc&#10;        try:&#10;            pipe.unet.load_attn_procs(str(repo_dir), weight_name=weight_name)&#10;            return&#10;        except Exception as exc:  # noqa: BLE001&#10;            last_error = exc&#10;        # Fallback: state dict injection for weights produced by our trainer&#10;        try:&#10;            state = load_file(str(lora_path), device=&quot;cpu&quot;)&#10;&#10;            # Reconstruct the attention processors from the state dict&#10;            attn_procs: Dict[str, torch.nn.Module] = {}&#10;            lora_cls = get_lora_processor_class()&#10;&#10;            # Use actual processor names from the UNet, not config&#10;            attn_processor_keys = list(pipe.unet.attn_processors.keys())&#10;&#10;            for name in attn_processor_keys:&#10;                cross_attention_dim = pipe.unet.config.cross_attention_dim if name.endswith(&quot;attn2.processor&quot;) else None&#10;                # Estimate sizes from saved tensors&#10;                proc_state_dict = {}&#10;                safe_name = name.replace(&quot;.&quot;, &quot;_&quot;)&#10;                for key, value in state.items():&#10;                    head, sep, tail = key.partition(&quot;.&quot;)&#10;                    if not sep:&#10;                        continue&#10;                    if head == safe_name or head.startswith(safe_name + &quot;_&quot;):&#10;                        proc_state_dict[tail] = value&#10;&#10;                if not proc_state_dict:&#10;                    continue&#10;&#10;                # Infer rank and dims&#10;                inferred_rank: Optional[int] = None&#10;                inferred_hidden: Optional[int] = None&#10;                inferred_cross: Optional[int] = None&#10;                for k, v in proc_state_dict.items():&#10;                    if not hasattr(v, &quot;shape&quot;):&#10;                        continue&#10;                    if k.endswith(&quot;down.weight&quot;) and len(v.shape) == 2:&#10;                        r, dim = int(v.shape[0]), int(v.shape[1])&#10;                        inferred_rank = inferred_rank or r&#10;                        lower = k.lower()&#10;                        if &quot;_q.&quot; in lower or &quot;to_q&quot; in lower:&#10;                            inferred_hidden = inferred_hidden or dim&#10;                        elif &quot;_k.&quot; in lower or &quot;to_k&quot; in lower:&#10;                            inferred_cross = inferred_cross or dim&#10;                        elif &quot;_v.&quot; in lower or &quot;to_v&quot; in lower:&#10;                            inferred_cross = inferred_cross or dim&#10;                        elif &quot;out.&quot; in lower or &quot;to_out&quot; in lower:&#10;                            inferred_hidden = inferred_hidden or dim&#10;                rank = inferred_rank or 4&#10;                hidden_size = inferred_hidden or _estimate_hidden_size(name, pipe.unet.config.block_out_channels)&#10;                cross_dim = inferred_cross if (inferred_cross is not None) else cross_attention_dim&#10;&#10;                # Create processor with inferred dimensions&#10;                try:&#10;                    lora_processor = lora_cls(&#10;                        hidden_size=hidden_size,&#10;                        cross_attention_dim=cross_dim,&#10;                        rank=rank,&#10;                        alpha=rank,&#10;                    )&#10;                except TypeError:&#10;                    lora_processor = lora_cls(&#10;                        hidden_size=hidden_size,&#10;                        cross_attention_dim=cross_dim,&#10;                        r=rank,  # type: ignore[arg-type]&#10;                        network_alpha=rank,  # type: ignore[arg-type]&#10;                    )&#10;&#10;                lora_processor.load_state_dict(proc_state_dict, strict=False)&#10;                attn_procs[name] = lora_processor&#10;&#10;            if not attn_procs:&#10;                raise RuntimeError(&quot;Could not find any valid LoRA layers in the state dictionary.&quot;)&#10;&#10;            pipe.unet.set_attn_processor(attn_procs)&#10;&#10;        except Exception as exc:  # noqa: BLE001&#10;            raise RuntimeError(&#10;                f&quot;无法加载 LoRA 权重 {lora_path}: {exc}\n最近一次 diffusers 加载错误: {last_error}&quot;&#10;            ) from exc&#10;&#10;    def build_pipeline(self, lora_path: Path) -&gt; StableDiffusionControlNetPipeline:&#10;        control_nets = self._load_controlnets()&#10;        pipe = self._build_base_pipe(control_nets)&#10;        self._configure_scheduler(pipe)&#10;        self._load_lora(pipe, lora_path)&#10;        pipe.enable_model_cpu_offload()&#10;        return pipe&#10;&#10;    # ------------------------------------------------------------------&#10;    # Conditioning utilities&#10;    # ------------------------------------------------------------------&#10;    def _ensure_lengths(self) -&gt; None:&#10;        n = len(self.cfg.controlnet_models)&#10;        if len(self.cfg.controlnet_modalities) != n:&#10;            raise ValueError(&quot;controlnet_modalities 数量需与 controlnet_models 保持一致。&quot;)&#10;        if len(self.cfg.control_scales) != n:&#10;            raise ValueError(&quot;control_scales 数量需与 controlnet_models 保持一致。&quot;)&#10;&#10;    def _load_condition_images(&#10;        self,&#10;        stem: str,&#10;        conditioning: Dict[str, Dict[str, Path]],&#10;    ) -&gt; List[Image.Image]:&#10;        if stem not in conditioning:&#10;            raise KeyError(f&quot;缺少样本 {stem} 的引导文件。&quot;)&#10;        images: List[Image.Image] = []&#10;        entry = conditioning[stem]&#10;        for modality in self.cfg.controlnet_modalities:&#10;            if modality not in entry:&#10;                raise KeyError(f&quot;样本 {stem} 缺少模态 {modality} 的引导图。&quot;)&#10;            images.append(Image.open(entry[modality]).convert(&quot;RGB&quot;))&#10;        return images&#10;&#10;    # ------------------------------------------------------------------&#10;    # Public API&#10;    # ------------------------------------------------------------------&#10;    def generate(&#10;        self,&#10;        lora_path: Path,&#10;        prompt_items: List[tuple[str, str]],&#10;        conditioning: Dict[str, Dict[str, Path]],&#10;        output_dir: Path,&#10;    ) -&gt; List[Path]:&#10;        self._ensure_lengths()&#10;        pipe = self.build_pipeline(lora_path)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;        results: List[Path] = []&#10;        for stem, prompt in track(prompt_items, description=&quot;Generating&quot;):&#10;            ctrl_images = self._load_condition_images(stem, conditioning)&#10;            # If only one ControlNet is in use, we must pass a single image instead of a list&#10;            cond_image = ctrl_images[0] if len(ctrl_images) == 1 else ctrl_images&#10;            image = pipe(&#10;                prompt,&#10;                num_inference_steps=self.cfg.num_inference_steps,&#10;                guidance_scale=self.cfg.guidance_scale,&#10;                image=cond_image,&#10;                controlnet_conditioning_scale=self.cfg.control_scales,&#10;            ).images[0]&#10;            out_path = output_dir / f&quot;{stem}.png&quot;&#10;            image.save(out_path)&#10;            results.append(out_path)&#10;&#10;        pipe.to(&quot;cpu&quot;)&#10;        torch.cuda.empty_cache()&#10;        return results&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>